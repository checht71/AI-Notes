TL;DR: JAX enables hardware acceleration for NumPy and allows for heavier computations.

JAX is a heavy lifter that squeezes performance out of custom neural nets.

Jax is a machine learning framework much like Tensorflow and PyTorch, specifically for ML research.
Tensorflow and PyTorch are great frameworks for machine learning, but they have their limitations. Researchers who want to build their neural networks completely from scratch usually do so using NumPy. FLAX is a way to integrate JAX with PyTorch and Tensorflow to speed them up.