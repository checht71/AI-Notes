[Has Generative AI Already Peaked? - Computerphile](https://www.youtube.com/watch?v=dDUC-LqVrPU)

Zero-shot learning is when an AI understands a concept that it has NEVER seen before. An example of this would be a model trained on cats and dogs only recognizing an elephant. 

The idea of all encompassing generative AI has some issues. It seems based on experimental evidence that the performance of an AI model has a logarithmic relationship to the number of samples in your training set. In layman's terms, more data does not make for a better model once you have a certain amount of data. Performance will plateau. This is contrary to the AI hype and optimism we have been exposed to over the past half decade. 

Class imbalance is also a huge challenge for AI.


There are also some great points in the comments on the current limitations of AI: 

> As a bioinformatician, I will always assume that the exponential growth will plateau sooner rather than later. Sure, new architectures may cause some expected exponential growth for a while, but they will find their ceiling quite fast.

> The data we have is actually incredibly limited. We only use mostly 2D image data. But in the real world, a cat is an animal. We perceive it in a 3D space with all of our senses, observe its behavior over time, compare it all to other animals, and influence its behavior over time. All of that, and more, makes a cat a cat. No AI has such kind of data.

